# PandaDoc Voice Agent - Complete Data Flow Specification

## Overview
This document provides an exhaustive breakdown of all data, metrics, and analytics flowing through the separated architecture system. Each section details exactly what data is collected, processed, and distributed to various systems.

---

## 1. LiveKit Agent (Data Collection Layer)

The LiveKit agent focuses solely on collecting raw data during voice sessions without performing any analysis.

### 1.1 Session Metadata
- **session_id**
  - Format: Room name string
  - Purpose: Unique identifier for the session
  - Used for: Tracking, deduplication, linking across systems
- **start_time**
  - Format: ISO 8601 timestamp
  - Purpose: Session initiation time
- **end_time**
  - Format: ISO 8601 timestamp
  - Purpose: Session completion time
- **duration_seconds**
  - Format: Integer
  - Calculation: end_time - start_time
  - Purpose: Total session length for metrics
- **participant_id**
  - Format: String (LiveKit participant ID)
  - Purpose: Unique participant identifier
- **user_email**
  - Source: Participant metadata
  - Format: Email string
  - Purpose: User identification, CRM matching
- **user_name**
  - Source: Participant metadata
  - Format: String
  - Purpose: Display name, personalization

### 1.2 Raw Transcript Data
- **Complete conversation history**
  - Format: JSON array of conversation items
  - Includes: All user and agent messages in chronological order
- **User utterances**
  - Content: Transcribed user speech
  - Timestamp: When utterance was spoken
  - Final flag: Whether transcription is final or interim
  - Speaker ID: User identifier
- **Agent responses**
  - Content: Agent's spoken responses
  - Timestamp: When response was generated
  - Source: Whether user-initiated or agent-initiated
- **Interruption markers**
  - Boolean flags indicating when user interrupted agent
  - Count: Total interruptions in session
  - Timestamps: When each interruption occurred
- **Turn-taking data**
  - Turn count: Total conversation turns
  - Average turn duration
  - Turn balance: Ratio of user vs agent speaking time

### 1.3 Performance Metrics (LiveKit Built-in)
- **eou_delay**
  - Description: End-of-utterance delay (milliseconds)
  - Purpose: Measures turn detection speed
  - Target: <300ms for natural conversation
- **llm_ttft**
  - Description: LLM time to first token (milliseconds)
  - Purpose: Measures thinking speed
  - Target: <700ms for responsive feel
- **tts_ttfb**
  - Description: TTS time to first byte (milliseconds)
  - Purpose: Measures speech generation start latency
  - Target: <300ms for immediate response
- **stt_duration**
  - Description: Speech-to-text processing time (milliseconds)
  - Purpose: Measures transcription speed
- **total_latency**
  - Calculation: eou_delay + llm_ttft + tts_ttfb
  - Purpose: End-to-end response time
  - Target: <1500ms total
- **prompt_tokens**
  - Description: LLM input token count
  - Purpose: Cost tracking, context monitoring
- **completion_tokens**
  - Description: LLM output token count
  - Purpose: Cost tracking, verbosity monitoring
- **total_tokens**
  - Calculation: prompt_tokens + completion_tokens
  - Purpose: Total LLM usage per turn
- **stt_audio_duration**
  - Description: Length of audio processed by STT (seconds)
  - Purpose: Input duration tracking
- **tts_characters_count**
  - Description: Number of characters sent to TTS
  - Purpose: Output length tracking
- **tts_audio_duration**
  - Description: Length of audio generated by TTS (seconds)
  - Purpose: Output duration tracking

### 1.4 Discovered Signals (Business Context)
Signals discovered during conversation through natural dialogue, not direct questions.

- **team_size**
  - Type: Integer or null
  - Range: 1-1000+
  - Discovery: From workflow discussion, role mentions
  - Qualification Impact: ‚â•5 = Enterprise tier
- **monthly_volume**
  - Type: Integer or null
  - Unit: Documents per month
  - Discovery: From frequency discussions
  - Qualification Impact: ‚â•100 = Enterprise tier
- **integration_needs**
  - Type: Array of strings
  - Values: ["salesforce", "hubspot", "api", "zapier", etc.]
  - Discovery: From workflow automation questions
  - Qualification Impact: Salesforce/HubSpot = High value
- **industry**
  - Type: String or null
  - Examples: "healthcare", "legal", "real estate", "finance"
  - Discovery: From use case discussions
  - Purpose: Segmentation, personalization
- **location**
  - Type: String or null
  - Format: City, state, country
  - Discovery: From timezone, mentions, metadata
  - Purpose: Territory routing, localization
- **use_case**
  - Type: String or null
  - Examples: "proposals", "contracts", "quotes", "NDAs"
  - Discovery: From document type discussions
  - Purpose: Solution mapping, feature recommendations
- **current_tool**
  - Type: String or null
  - Examples: "manual", "DocuSign", "Adobe Sign", "PandaDoc"
  - Discovery: From workflow pain points
  - Purpose: Competitive intelligence, migration planning
- **pain_points**
  - Type: Array of strings
  - Examples: ["slow turnaround", "no tracking", "manual follow-up"]
  - Discovery: From problem discussions
  - Purpose: Solution positioning
- **decision_timeline**
  - Type: String or null
  - Values: "this_week", "this_month", "this_quarter", "evaluating"
  - Discovery: From timeline questions
  - Purpose: Urgency scoring, follow-up timing
- **budget_authority**
  - Type: String or null
  - Values: "decision_maker", "influencer", "needs_approval"
  - Discovery: From role, approval discussions
  - Purpose: BANT qualification
- **urgency**
  - Type: String or null
  - Values: "high", "medium", "low"
  - Discovery: From tone, timeline, pain severity
  - Purpose: Prioritization
- **qualification_tier**
  - Type: String
  - Values: "Tier 1" (sales-ready) or "Tier 2" (self-serve)
  - Calculation: Based on team_size, volume, integrations
  - Purpose: Routing decision

### 1.5 Tool Usage Tracking

#### unleash_search_knowledge calls
Each knowledge base search is tracked with:
- **query**
  - Type: String
  - Content: User's exact question passed to search
- **category**
  - Type: String or null
  - Values: "features", "pricing", "integrations", "troubleshooting"
  - Purpose: Filter classification
- **results_found**
  - Type: Boolean
  - Purpose: Success/failure tracking
- **total_results**
  - Type: Integer
  - Purpose: Result quantity for content gap analysis
- **timestamp**
  - Type: ISO 8601 timestamp
  - Purpose: Temporal analysis

#### book_sales_meeting calls
Each meeting booking attempt is tracked with:
- **customer_name**
  - Type: String
  - Content: Full name provided
- **customer_email**
  - Type: String (email format)
  - Purpose: Calendar invite, CRM matching
- **preferred_date**
  - Type: String or null
  - Content: Natural language date ("tomorrow", "next Tuesday")
- **preferred_time**
  - Type: String or null
  - Content: Natural language time ("2pm", "morning")
- **success**
  - Type: Boolean
  - Purpose: Booking success tracking
- **meeting_link**
  - Type: URL string or null
  - Content: Google Meet/Calendar link if successful
- **calendar_event_id**
  - Type: String or null
  - Content: Google Calendar event ID
- **timestamp**
  - Type: ISO 8601 timestamp
  - Purpose: Temporal analysis

### 1.6 Connection Quality Metrics
- **bandwidth_in**
  - Type: Integer (bytes)
  - Purpose: Downstream bandwidth usage
  - Use: Cost optimization, quality correlation
- **bandwidth_out**
  - Type: Integer (bytes)
  - Purpose: Upstream bandwidth usage
  - Use: Cost optimization, quality correlation
- **packet_loss**
  - Type: Float (percentage)
  - Purpose: Network quality indicator
  - Target: <1% for good quality
- **jitter**
  - Type: Integer (milliseconds)
  - Purpose: Network stability indicator
  - Target: <30ms for smooth audio
- **device_info**
  - Browser: User agent string
  - OS: Operating system and version
  - Location: Geographic region (from GeoIP)
  - Purpose: Device/network correlation, support

---

## 2. Message Queue (Data Transport)

### 2.1 Queue Configuration
- **Platform**: AWS SQS or Google Pub/Sub
- **Message Format**: JSON
- **Message Size**: Up to 256KB (SQS) or 10MB (Pub/Sub)

### 2.2 Message Structure
- **Body**: Complete raw session data (all sections above)
- **Message Attributes**:
  - **session_id**: For deduplication and tracking
  - **priority**: "hot_lead" vs "standard" for prioritization
  - **timestamp**: Queue entry time
  - **user_email**: For filtering/routing (optional)

### 2.3 Reliability Configuration
- **Retry Policy**: 3 attempts with exponential backoff (1s, 2s, 4s)
- **Visibility Timeout**: 5 minutes (processing time limit)
- **Dead Letter Queue**: Failed messages after 3 retries
- **Message Retention**: 14 days
- **Deduplication**: 5-minute deduplication window (for SQS FIFO)

---

## 3. Analytics Agent (Processing Pipeline)

The analytics agent processes raw session data through multiple specialized modules.

### 3.1 Sales Intelligence Analyzer

#### Extractions Performed:
- **Team size extraction & validation**
  - NLP: Extract numbers mentioned with "team", "users", "people"
  - Validation: Cross-reference with role counts, workflow complexity
  - Confidence scoring: High if multiple indicators agree
- **Document volume estimation**
  - NLP: Extract frequency mentions ("daily", "50/month", etc.)
  - Conversion: Normalize to documents/month
  - Validation: Check against team size reasonableness
- **CRM integration detection**
  - Pattern matching: "Salesforce", "HubSpot", "CRM", "API"
  - Context: Distinguish between current use vs need
  - Classification: Specific CRM vs generic need
- **Budget authority inference**
  - Role detection: "VP", "Director", "Manager", "Owner"
  - Language patterns: "I can decide", "need approval", etc.
  - Classification: Decision maker, influencer, or evaluator
- **Decision timeline parsing**
  - Temporal extraction: "this week", "by end of month", "Q1"
  - Urgency inference: "urgent", "ASAP", "when possible"
  - Classification: this_week, this_month, this_quarter, evaluating
- **Competitor mention detection**
  - Entity recognition: DocuSign, Adobe Sign, HelloSign, etc.
  - Context: Current tool vs consideration set
  - Sentiment: Positive, negative, or neutral towards competitor
- **Objection classification**
  - Category detection: Pricing, features, timing, trust
  - Severity scoring: Blocker vs concern
  - Resolution status: Addressed vs unresolved
- **Feature request extraction**
  - Request detection: "wish I could", "need to", "looking for"
  - Feature identification: Specific capability mentioned
  - Priority inference: Must-have vs nice-to-have
- **Buying signal detection**
  - Explicit: "Let's do it", "how do I buy", "next steps"
  - Implicit: Timeline urgency, ROI calculations, stakeholder mentions
  - Scoring: Strong, medium, weak signal
- **Stakeholder identification**
  - Role extraction: Who else is involved
  - Influence level: Decision maker, gatekeeper, user
  - Engagement: Will they be on calls, need to be sold
- **Use case categorization**
  - Primary use: Sales proposals, legal contracts, HR docs, etc.
  - Secondary uses: Other document types mentioned
  - Complexity: Single vs multi-use case
- **Pain point analysis**
  - Problem extraction: Current frustrations, inefficiencies
  - Impact assessment: How much time/money lost
  - Solution mapping: Which PandaDoc features address each
- **BANT qualification scoring**
  - Budget: Authority level, budget discussions
  - Authority: Decision maker identified
  - Need: Pain points severity and frequency
  - Timeline: Urgency and decision date
  - Overall: Pass/fail on BANT criteria

#### Output: Sales Intelligence Report
JSON structure with all above findings plus confidence scores and supporting quotes.

### 3.2 Lead Scoring Engine

#### Score Components:
- **Demographic score (0-20 points)**
  - Company size indicators
  - Industry vertical
  - Location/region
- **Behavioral score (0-25 points)**
  - Engagement level (turn count, questions asked)
  - Call duration appropriateness
  - Feature exploration depth
  - Active participation vs passive listening
- **Firmographic score (0-20 points)**
  - Revenue indicators (if mentioned)
  - Employee count
  - Market presence signals
- **Intent score (0-20 points)**
  - Timeline urgency
  - Budget discussions
  - Next steps commitment
  - Meeting willingness
- **Fit score (0-15 points)**
  - ICP matching (team size, volume, use case)
  - Integration needs alignment
  - Use case complexity match
- **Weighted total score (0-100)**
  - Sum of all component scores
  - Threshold: >80 = hot lead, 60-80 = warm, <60 = nurture
- **Confidence level**
  - High: Multiple strong signals, clear indicators
  - Medium: Some signals, some ambiguity
  - Low: Limited data, high uncertainty

#### Output: Lead Score Report
- Total score (0-100)
- Component breakdown with individual scores
- Confidence level
- Score justification with key factors
- Historical score (if returning user)
- Score change explanation (if applicable)

### 3.3 Conversation Quality Analyzer

#### Quality Metrics:
- **Turn balance ratio**
  - Calculation: User turns / Agent turns
  - Target: 0.6-1.4 (balanced conversation)
  - Issue detection: >2.0 = agent dominating, <0.5 = user disengaged
- **Question engagement count**
  - User questions: How many questions did user ask
  - Agent questions: How many questions did agent ask
  - Target: 5-12 total questions for engaged conversation
- **Active listening indicators**
  - Acknowledgments: "I see", "got it", "mm-hmm" from agent
  - Build-on responses: Agent referencing previous user statements
  - Clarifications: Agent asking for clarification
- **Interruption frequency**
  - Count: Total user interruptions of agent
  - Rate: Interruptions per minute
  - Target: <3 total, <0.5/min
  - Context: Excitement interruptions vs frustration
- **Dead air percentage**
  - Calculation: Total silence time / Call duration
  - Target: <5% dead air
  - Detection: Gaps >3 seconds between turns
- **Natural ending detection**
  - Proper goodbye: Agent and user say goodbye
  - Resolution: Issues/questions resolved
  - Next steps: Clear action items established
  - Vs abrupt ending: Hangup without goodbye
- **Sentiment progression (start ‚Üí end)**
  - Start sentiment: First 25% of conversation
  - End sentiment: Last 25% of conversation
  - Trend: Improving, stable, declining
  - Target: Positive or improving trend
- **User satisfaction proxy**
  - Sentiment + engagement + resolution
  - Proxy score 0-10 for satisfaction
- **Agent effectiveness score**
  - Question resolution rate
  - Response appropriateness
  - Tool usage effectiveness
  - Overall score 0-10

#### Output: Quality Score Report
- Overall quality score (0-10)
- Component scores with targets and actuals
- Red flags (dead air, interruptions, negative trend)
- Strengths (engagement, resolution, sentiment)
- Improvement recommendations

### 3.4 Sentiment Analyzer

#### Sentiment Dimensions:
- **Overall sentiment**
  - Classification: Positive, neutral, negative
  - Confidence: 0-1 score
  - Method: Average across all utterances
- **Sentiment by conversation segment**
  - Opening (first 25%)
  - Discovery (next 25%)
  - Discussion (next 25%)
  - Closing (last 25%)
  - Trend analysis: Improving, declining, stable
- **Emotional tone detection**
  - Emotions: Excitement, frustration, confusion, confidence, hesitation
  - Intensity: Low, medium, high
  - Triggers: What caused emotion shifts
- **Frustration indicators**
  - Keywords: "frustrated", "annoying", "difficult"
  - Tone markers: Increased pitch, faster speech (from audio)
  - Repetition: Asking same question multiple times
- **Enthusiasm markers**
  - Keywords: "great", "perfect", "love it", "exactly"
  - Exclamations: "Wow!", "That's awesome!"
  - Engagement: Lots of follow-up questions
- **Confidence level**
  - Analysis confidence: How certain is the sentiment analysis
  - User confidence: How confident does user seem in their needs/decision

#### Output: Sentiment Report
- Overall sentiment with score
- Sentiment progression chart data
- Key emotion moments with timestamps
- Frustration/enthusiasm triggers
- Recommendations for follow-up tone

### 3.5 Objection & Competitor Detector

#### Objection Types:
- **Pricing objections**
  - Explicit: "Too expensive", "over budget", "can't afford"
  - Implicit: Price comparison requests, discount inquiries
  - Severity: Blocker vs concern
  - Status: Resolved vs outstanding
- **Feature gap objections**
  - Missing features: "Doesn't have X", "need Y feature"
  - Specific capabilities: Integration, automation, etc.
  - Workarounds: Can it be solved another way
- **Timing objections**
  - "Not right now", "maybe later", "check back in Q2"
  - Reason: Budget cycle, other priorities, evaluation phase
- **Competitor mentions**
  - Competitors detected: DocuSign, Adobe Sign, HelloSign, PandaDoc competitors
  - Context: "Using DocuSign now", "considering Adobe", "saw HelloSign"
  - Sentiment: Positive, negative, neutral
- **Competitor comparison requests**
  - "How does this compare to DocuSign?"
  - Feature comparison: Specific features vs competitor
  - Pricing comparison: Cost questions vs competitor
- **Switching barriers identified**
  - Technical: Integrations, data migration
  - Contractual: Existing contract, cancellation terms
  - Organizational: Training, change management, internal politics
  - Cost: Switching costs, migration effort

#### Output: Objection & Competitor Report
- All objections categorized and prioritized
- Competitor intelligence summary
- Comparison talking points needed
- Switching barrier mitigation strategies
- Recommended battlecard triggers

### 3.6 Performance Metrics Calculator

#### Calculations:
- **Average latency**
  - Mean of all total_latency measurements
  - Target: <1000ms average
- **p50, p95, p99 latency percentiles**
  - p50 (median): Middle value
  - p95: 95th percentile (outlier threshold)
  - p99: 99th percentile (worst case)
  - Target: p95 <1500ms, p99 <2000ms
- **Token usage statistics**
  - Total tokens: Sum across all turns
  - Average tokens per turn
  - Token distribution: Prompt vs completion
  - Cost per token tier (GPT-4, GPT-3.5, etc.)
- **Cost calculation (per provider)**
  - LLM costs: Tokens √ó price per token
  - STT costs: Audio minutes √ó price per minute
  - TTS costs: Characters √ó price per character
  - LiveKit costs: Minutes √ó price per minute
  - Total session cost
- **Performance score vs targets**
  - Latency score: How close to <1000ms target
  - Reliability score: Error rate
  - Efficiency score: Token usage appropriateness
  - Overall performance score 0-10
- **Anomaly detection**
  - Latency spikes: >2x normal
  - Token usage spikes: >2x normal
  - Error bursts: Multiple errors in short time
  - Pattern recognition: Recurring issues

#### Output: Performance Report
- All latency stats with targets
- Cost breakdown by provider
- Performance score and grade
- Anomalies detected with timestamps
- Optimization recommendations

### 3.7 Content Gap Analyzer

#### Gap Identification:
- **Failed knowledge base searches**
  - Query: What was searched
  - Results: How many results (0 = gap)
  - Frequency: How often this query fails
  - Impact: How critical to user journey
- **Unanswered questions**
  - Question detection: User asked but no clear answer
  - Topic: What subject area
  - Frequency: Common question patterns
  - Deflection: Did user leave unsatisfied
- **Repeat question patterns**
  - Clustering: Group similar questions
  - Frequency ranking: Most common gaps
  - Priority: Which to address first
- **Missing documentation areas**
  - Topics: Features/topics with no coverage
  - Source gaps: Missing Intercom articles
  - Integration docs: Missing integration guides
- **Source effectiveness (Intercom coverage)**
  - Coverage rate: % of queries with results
  - Quality: Result relevance (implied by follow-ups)
  - Recency: Age of content
  - Gaps: Topics not covered by Intercom source

#### Output: Content Gap Report
- Top 10 failed searches
- Question clusters needing documentation
- Priority ranking for content creation
- Specific Intercom article recommendations
- Content type recommendations (guide vs troubleshooting)

### 3.8 Action Item Extractor

#### Action Items:
- **Commitments made by user**
  - "I'll send you the contract"
  - "Let me check with my team"
  - "I'll try this feature tomorrow"
  - Deadline: When committed to
- **Follow-up tasks for sales**
  - "Send pricing comparison"
  - "Schedule demo call"
  - "Provide case study"
  - Owner: Who should do this
  - Deadline: When needed by
- **Feature demo requests**
  - Specific features to demo
  - Use case for demo
  - Preferred format (video, live, doc)
- **Document sharing needs**
  - What documents: Case studies, pricing, contracts
  - Format: PDF, link, presentation
  - Timing: When needed
- **Next steps timeline**
  - When: User's timeline for next actions
  - What: Specific next steps agreed
  - Who: Owners for each action

#### Output: Action Items List
- Structured list of all action items
- Owner assignments (user, sales, support)
- Deadlines and priorities
- Context for each item
- Tracking IDs for follow-up

### 3.9 Executive Summary Generator

#### Summary Components:
- **Call outcome**
  - Qualified or unqualified
  - Tier classification
  - Next action: Sales call, self-serve, nurture
- **Key takeaways (3-5 bullets)**
  - Most important insights
  - Decision drivers
  - Unique circumstances
- **Risk factors**
  - Objections not resolved
  - Competitor considerations
  - Budget constraints
  - Timeline mismatches
- **Opportunity summary**
  - Deal size potential
  - Expansion opportunities
  - Referral potential
  - Strategic value
- **Recommended next action**
  - Specific action: Call, email, demo, quote
  - Owner: Who should do it
  - Timing: When to do it
  - Talking points: What to emphasize

#### Output: Executive Summary
- 2-3 paragraph summary
- Bulleted key takeaways
- Risk/opportunity assessment
- Clear next action with owner and timing
- CXO-ready format (suitable for forwarding to executives)

---

## 4. Salesforce CRM (Sales Intelligence Destination)

All analyzed data is synced to Salesforce for sales team use.

### 4.1 Lead/Contact Record Updates (Standard Fields)
- **Lead Score**
  - API Name: `Lead_Score__c`
  - Type: Number(3,0)
  - Range: 0-100
  - Update: Every call updates score
  - Display: Prominently on record page
- **Lead Score Breakdown**
  - API Name: `Lead_Score_Breakdown__c`
  - Type: Long Text Area (JSON)
  - Content: Component scores and justification
  - Use: Transparency for sales reps
- **Qualification Tier**
  - API Name: `Qualification_Tier__c`
  - Type: Picklist
  - Values: "Tier 1 - Sales Ready", "Tier 2 - Self Serve"
  - Impact: Routing, prioritization
- **Last Call Date**
  - API Name: `Last_Voice_Call_Date__c`
  - Type: DateTime
  - Update: Every call
  - Use: Recency tracking
- **Call Duration**
  - API Name: `Last_Call_Duration_Minutes__c`
  - Type: Number(4,1)
  - Format: Minutes with 1 decimal
  - Use: Engagement metric
- **Engagement Score**
  - API Name: `Voice_Engagement_Score__c`
  - Type: Number(3,1)
  - Range: 0.0-10.0
  - Update: Latest call score
  - Use: Quality metric

### 4.2 Discovered Signals (Custom Fields)
- **Team_Size__c**
  - Type: Number(4,0)
  - Range: 1-9999
  - Validation: Must be positive integer
  - Impact: Routing rule (‚â•5 to AE team)
- **Monthly_Document_Volume__c**
  - Type: Number(5,0)
  - Range: 0-99999
  - Unit: Documents per month
  - Impact: Tier classification
- **Integration_Needs__c**
  - Type: Multi-Select Picklist
  - Values: Salesforce, HubSpot, Zapier, API, Google Drive, Dropbox, etc.
  - Selection: Multiple allowed
  - Impact: Solution Engineer involvement
- **Industry_Vertical__c**
  - Type: Picklist
  - Values: Healthcare, Legal, Real Estate, Finance, Sales, HR, etc.
  - Purpose: Segmentation, case study matching
- **Decision_Timeline__c**
  - Type: Picklist
  - Values: This Week, This Month, This Quarter, Next Quarter, Evaluating
  - Impact: Follow-up timing, priority
- **Budget_Authority__c**
  - Type: Picklist
  - Values: Decision Maker, Influencer, Needs Approval, Unknown
  - Purpose: BANT scoring, meeting preparation
- **Current_Tool__c**
  - Type: Text(255)
  - Content: Current document tool (Manual, DocuSign, etc.)
  - Purpose: Competitive intelligence, migration planning
- **Urgency_Level__c**
  - Type: Picklist
  - Values: High, Medium, Low
  - Impact: Follow-up priority, SLA

### 4.3 Pain Points & Needs
- **Pain_Points__c**
  - Type: Long Text Area (32,000 chars)
  - Format: Comma-separated or bulleted list
  - Content: All pain points mentioned
  - Use: Solution positioning
- **Use_Cases__c**
  - Type: Multi-Select Picklist
  - Values: Sales Proposals, Contracts, Quotes, NDAs, Onboarding, etc.
  - Purpose: Feature recommendations
- **Feature_Requests__c**
  - Type: Long Text Area (32,000 chars)
  - Content: Features user asked about or requested
  - Use: Product feedback, roadmap prioritization

### 4.4 Competitive Intelligence
- **Competitor_Mentioned__c**
  - Type: Picklist
  - Values: DocuSign, Adobe Sign, HelloSign, Manual Process, Other, None
  - Purpose: Battlecard activation
- **Current_Solution__c**
  - Type: Text(255)
  - Content: More detailed current solution description
  - Purpose: Migration planning
- **Switching_Barriers__c**
  - Type: Long Text Area
  - Content: Identified barriers to switching
  - Use: Objection handling preparation

### 4.5 Call Intelligence
- **Objections_Raised__c**
  - Type: Long Text Area
  - Content: All objections from call
  - Format: Categorized list
  - Use: Objection handling preparation
- **Buying_Signals__c**
  - Type: Long Text Area
  - Content: Positive signals detected
  - Use: Confidence assessment, closing strategy
- **Stakeholders_Mentioned__c**
  - Type: Text(255)
  - Content: Other people involved in decision
  - Use: Multi-threading strategy
- **Key_Quotes__c**
  - Type: Long Text Area
  - Content: Important direct quotes from user
  - Use: Personalization, proposal writing
- **Call_Sentiment__c**
  - Type: Picklist
  - Values: Positive, Neutral, Negative
  - Purpose: Follow-up tone adjustment

### 4.6 Action Items & Next Steps
- **Next_Best_Action__c**
  - Type: Text(255)
  - Content: Recommended next step from AI
  - Use: Rep guidance
- **Follow_Up_Priority__c**
  - Type: Picklist
  - Values: High (within 24h), Medium (within week), Low (standard cadence)
  - Impact: Task prioritization
- **Recommended_Owner__c**
  - Type: Lookup(User)
  - Purpose: Assignment recommendation
  - Logic: Based on territory, availability, specialization

### 4.7 Automated Task Creation
When a qualified lead (Tier 1) has a call, automatically create:
- **Subject**: "Follow up on voice trial call - [Lead Name]"
- **Description**:
  - Executive summary
  - Key takeaways
  - Action items
  - Salesforce record link
  - Transcript link
- **Due Date**:
  - High urgency: Tomorrow
  - Medium urgency: +3 days
  - Low urgency: +7 days
- **Priority**: Mapped from Follow_Up_Priority__c
- **Status**: "Not Started"
- **Assigned To**:
  - Logic: Territory-based routing or round-robin
  - Fallback: Queue assignment

### 4.8 Custom Object: Voice_Call__c
Separate object to store call details:
- **Session_Id__c**
  - Type: Text(255), Unique, External ID
  - Purpose: Linking, deduplication
- **Transcript__c**
  - Type: Rich Text Area (Long)
  - Content: Full formatted transcript
  - Display: Tab on Lead/Contact record
- **Call_Summary__c**
  - Type: Long Text Area
  - Content: Executive summary
  - Use: Quick review without reading transcript
- **Transcript_URL__c**
  - Type: URL(255)
  - Link: S3/GCS location of full JSON
  - Purpose: Deep dive access
- **Quality_Score__c**
  - Type: Number(3,1)
  - Range: 0.0-10.0
  - Purpose: Call quality tracking
- **Call_Date__c**
  - Type: DateTime
  - Purpose: Chronological sorting
- **Duration_Minutes__c**
  - Type: Number(4,1)
  - Purpose: Length tracking
- **Related_To__c**
  - Type: Lookup(Lead, Contact)
  - Purpose: Relationship to person
- **Metrics_JSON__c**
  - Type: Long Text Area (JSON)
  - Content: All performance metrics
  - Purpose: Technical analysis

---

## 5. Amplitude Analytics (Product Analytics)

### 5.1 Session Completion Event
**Event Name**: `voice_trial_session_completed`

**Event Properties**:
- **session_id**: String (unique identifier)
- **duration_seconds**: Integer (call length)
- **turn_count**: Integer (total conversation turns)
- **qualification_tier**: String ("Tier 1" or "Tier 2")
- **engagement_score**: Number (0-10, quality score)
- **sentiment**: String ("positive", "neutral", "negative")
- **meeting_booked**: Boolean (true if sales meeting booked)
- **tool_calls_count**: Integer (total tool invocations)

**When Fired**: End of every voice session

### 5.2 User Identification
**User ID**: user_email or participant_id

**User Properties** (set/updated on each call):
- **team_size**: Integer or null
- **monthly_volume**: Integer or null
- **industry**: String or null
- **location**: String or null
- **qualification_tier**: String ("Tier 1", "Tier 2", or "Unknown")
- **lead_score**: Integer (0-100)
- **last_call_date**: Timestamp
- **total_calls**: Integer (incremented each call)

### 5.3 Engagement Events

#### Event: `knowledge_base_searched`
**Properties**:
- **query**: String (search query)
- **results_found**: Boolean (true if results returned)
- **total_results**: Integer (count of results)
- **category**: String or null (search category filter)
- **session_id**: String (link to session)

**When Fired**: Every unleash_search_knowledge tool call

#### Event: `sales_meeting_booked`
**Properties**:
- **meeting_time**: Timestamp (scheduled time)
- **preferred_date**: String (user's requested date)
- **preferred_time**: String (user's requested time)
- **success**: Boolean (booking success)
- **qualification_tier**: String (user's tier)
- **session_id**: String

**When Fired**: Every book_sales_meeting tool call

#### Event: `feature_discussed`
**Properties**:
- **feature_name**: String (specific feature mentioned)
- **interest_level**: String ("high", "medium", "low")
- **context**: String ("question", "demo_request", "objection")
- **session_id**: String

**When Fired**: When specific features are discussed (detected in analysis)

### 5.4 Quality Metrics Event

#### Event: `conversation_quality_measured`
**Properties**:
- **quality_score**: Number (0-10, overall quality)
- **turn_balance**: Number (user/agent turn ratio)
- **interruption_count**: Integer
- **dead_air_percentage**: Number (0-100)
- **natural_ending**: Boolean
- **session_id**: String

**When Fired**: End of session after quality analysis

### 5.5 Performance Metrics Event

#### Event: `latency_measured`
**Properties**:
- **avg_latency_ms**: Integer (average total latency)
- **p95_latency_ms**: Integer (95th percentile)
- **max_latency_ms**: Integer (worst latency in session)
- **avg_llm_ttft_ms**: Integer (average LLM thinking time)
- **avg_tts_ttfb_ms**: Integer (average speech start time)
- **total_tokens**: Integer (LLM token usage)
- **session_id**: String

**When Fired**: End of session after metric aggregation

---

## 6. Real-Time Dashboard (Looker/Tableau/Metabase)

### 6.1 Performance Monitoring Panel
**Widgets**:
- **Current calls active**
  - Type: Numeric counter
  - Update: Real-time
  - Source: LiveKit Cloud API
- **Average latency (last hour)**
  - Type: Numeric gauge
  - Target line: 1000ms
  - Color: Green <1000ms, Yellow 1000-1500ms, Red >1500ms
- **P95 latency trend**
  - Type: Line chart
  - Time range: Last 24 hours
  - Target line: 1500ms
- **Error rate**
  - Type: Percentage gauge
  - Calculation: Errors / Total calls
  - Alert threshold: >5%
- **Token usage rate**
  - Type: Line chart
  - Unit: Tokens per minute
  - Cost projection: $/hour display
- **Cost burn rate**
  - Type: Numeric counter ($/hour)
  - Breakdown: By provider (OpenAI, Deepgram, ElevenLabs)
  - Budget tracking: % of daily/monthly budget
- **Connection quality distribution**
  - Type: Pie chart
  - Categories: Excellent, Good, Fair, Poor
  - Based on: Bandwidth, packet loss, jitter

### 6.2 Business Metrics Panel
**Widgets**:
- **Today's calls**
  - Type: Numeric counter
  - Comparison: vs yesterday, vs last week
- **Qualified leads (Tier 1)**
  - Type: Numeric counter
  - Percentage: % of total calls
  - Trend: Up/down from yesterday
- **Meetings booked**
  - Type: Numeric counter
  - Conversion rate: % of qualified leads
- **Average engagement score**
  - Type: Numeric gauge (0-10)
  - Distribution: Histogram of all scores
- **Conversion rate**
  - Type: Percentage gauge
  - Calculation: Qualified / Total calls
  - Target: Configurable goal line
- **Revenue pipeline influenced**
  - Type: Currency counter
  - Source: Salesforce opportunities with voice call
  - Trend: Running total this month/quarter

### 6.3 Call Quality Panel
**Widgets**:
- **Average call duration**
  - Type: Numeric display (minutes)
  - Distribution: Histogram
  - Target range: 3-7 minutes highlighted
- **Average turn count**
  - Type: Numeric display
  - Distribution: Histogram
  - Target range: 8-15 turns highlighted
- **Interruption rate**
  - Type: Percentage display
  - Calculation: Calls with >3 interruptions / Total
  - Trend: Over time
- **Sentiment distribution**
  - Type: Pie chart
  - Categories: Positive, Neutral, Negative
  - Goal: >70% positive
- **Natural ending rate**
  - Type: Percentage display
  - Calculation: Calls with natural ending / Total
  - Goal: >90%
- **User satisfaction proxy**
  - Type: Numeric gauge (0-10)
  - Calculation: Composite of engagement + sentiment + resolution
  - Trend: Over time

### 6.4 Knowledge Base Effectiveness Panel
**Widgets**:
- **Search success rate**
  - Type: Percentage display
  - Calculation: Searches with results / Total searches
  - Target: >85%
- **Top searched queries**
  - Type: Table
  - Columns: Query, Count, Success Rate, Avg Results
  - Sorting: By frequency
  - Highlighting: Failed searches in red
- **Content gaps identified**
  - Type: Numeric counter
  - Definition: Queries with 0 results
  - Action link: To content creation board
- **Average results per query**
  - Type: Numeric display
  - Distribution: Histogram
  - Trend: Over time
- **Tool usage breakdown**
  - Type: Pie chart
  - Tools: unleash_search_knowledge, book_sales_meeting
  - Shows: Call count per tool

### 6.5 Sales Intelligence Panel
**Widgets**:
- **Lead score distribution**
  - Type: Histogram
  - Buckets: 0-20, 21-40, 41-60, 61-80, 81-100
  - Color: Heat map (cold to hot)
- **Top industries**
  - Type: Horizontal bar chart
  - Shows: Top 10 industries by call count
  - Drill-down: Click for industry details
- **Team size distribution**
  - Type: Bar chart
  - Buckets: 1-2, 3-4, 5-10, 11-25, 26+
  - Highlight: Enterprise tier (‚â•5)
- **Integration needs breakdown**
  - Type: Bar chart
  - Shows: Most requested integrations
  - Color: By tier (Tier 1 vs Tier 2)
- **Competitor mentions**
  - Type: Word cloud
  - Size: Frequency of mention
  - Color: By sentiment (green=negative about competitor, red=positive)
- **Common objections**
  - Type: Table
  - Columns: Objection, Count, Category, Resolution Rate
  - Action: Click for handling tips

### 6.6 Trending Insights Panel
**Widgets**:
- **Calls trend (last 7/30 days)**
  - Type: Line chart
  - Lines: Total calls, Qualified calls, Meetings booked
  - Annotations: Notable events
- **Qualification rate trend**
  - Type: Line chart
  - Y-axis: % of calls that are Tier 1
  - Target line: Goal qualification rate
- **Meeting booking trend**
  - Type: Line chart
  - Y-axis: % of qualified leads booking meetings
  - Target line: Goal booking rate
- **Performance degradation alerts**
  - Type: Alert list
  - Shows: Recent alerts with time, severity, status
  - Action: Click to investigate
- **Hot leads today**
  - Type: Ranked list (top 10)
  - Shows: Name, Score, Key Signals, Salesforce link
  - Refresh: Every 5 minutes
  - Action: Click to view full profile

### 6.7 Dashboard Filters (Global)
- **Date range**: Today, Yesterday, Last 7 days, Last 30 days, Custom
- **Qualification tier**: All, Tier 1, Tier 2
- **Sentiment**: All, Positive, Neutral, Negative
- **Industry**: All, [list of industries]
- **Agent version**: For A/B testing different agent versions

---

## 7. Alert & Notification System

### 7.1 Critical Alerts (Immediate - PagerDuty)
**Severity**: P1 (Critical)
**Response Time**: Immediate
**Escalation**: On-call engineer

#### Alert: Agent Down
- **Trigger**: No sessions started in last 10 minutes during business hours
- **Check**: Every 2 minutes
- **Message**: "Voice agent appears down - no sessions in 10 min"
- **Action**: Check LiveKit Cloud status, agent logs, deployment health

#### Alert: Error Rate >10%
- **Trigger**: Error rate exceeds 10% over 5-minute window
- **Calculation**: Failed sessions / Total sessions
- **Message**: "Critical error rate: X% errors in last 5 min"
- **Context**: Recent error messages, affected users
- **Action**: Check logs, rollback if recent deployment

#### Alert: Average Latency >2000ms
- **Trigger**: Average total latency >2000ms over 5-minute window
- **Message**: "Critical latency degradation: Xms average"
- **Context**: Breakdown by component (STT, LLM, TTS)
- **Action**: Check provider status pages, scaling

#### Alert: Queue Backlog >100
- **Trigger**: SQS/Pub/Sub message count >100
- **Message**: "Analytics queue backlog: X messages"
- **Context**: Queue age, processing rate
- **Action**: Scale up analytics workers, check for stuck messages

#### Alert: API Failures
- **Trigger**: Unleash, Salesforce, or other critical API down
- **Message**: "Critical API failure: [API name] returning errors"
- **Context**: Error rate, error messages
- **Action**: Check API status, implement fallback if available

### 7.2 High Priority Alerts (5 min - Slack #ops)
**Severity**: P2 (High)
**Response Time**: Within 15 minutes
**Channel**: Slack #ops with @here mention

#### Alert: Latency >1500ms Sustained
- **Trigger**: Average latency >1500ms for 10 minutes
- **Message**: "Performance degradation: Xms average latency"
- **Context**: Trend chart, component breakdown
- **Action**: Investigate slow components

#### Alert: Error Rate >5%
- **Trigger**: Error rate 5-10% over 10-minute window
- **Message**: "Elevated error rate: X%"
- **Context**: Error types, frequency
- **Action**: Monitor for escalation, investigate if sustained

#### Alert: Token Usage Spike
- **Trigger**: Token usage >2x normal rate
- **Message**: "Token usage spike: X tokens/min (normal: Y)"
- **Context**: Cost impact, affected sessions
- **Action**: Check for prompt issues, unexpected loops

#### Alert: Cost Burn Rate Exceeds Budget
- **Trigger**: Hourly cost rate projects over daily/monthly budget
- **Message**: "Cost alert: $X/hour rate projects $Y/month"
- **Context**: Cost breakdown by provider
- **Action**: Review usage patterns, consider optimizations

#### Alert: Connection Quality Degraded
- **Trigger**: >20% of calls in "Poor" quality category
- **Message**: "Connection quality issues: X% poor quality"
- **Context**: Geographic distribution, ISP patterns
- **Action**: Check for regional issues, CDN performance

### 7.3 Business Alerts (Real-time - Slack #sales)
**Severity**: Business Critical
**Response Time**: Within 1 hour (for hot leads)
**Channel**: Slack #sales with clickable links

#### Alert: Hot Lead Detected (Score >80)
- **Trigger**: Lead score >80 on call completion
- **Message Format**:
  ```
  üî• HOT LEAD ALERT
  Name: John Smith (john@acme.com)
  Company: Acme Corp
  Score: 87/100

  Key Signals:
  ‚Ä¢ Team size: 15 users
  ‚Ä¢ Volume: 200 docs/month
  ‚Ä¢ Needs: Salesforce integration
  ‚Ä¢ Timeline: This week

  üîó Salesforce: [link]
  üîó Transcript: [link]

  Recommended: Call within 2 hours
  ```
- **Assignee**: Territory owner auto-mentioned
- **Action**: Immediate follow-up call/email

#### Alert: Meeting Booked
- **Trigger**: book_sales_meeting tool succeeds
- **Message Format**:
  ```
  üìÖ SALES MEETING BOOKED
  Customer: John Smith (john@acme.com)
  Meeting: Tomorrow at 2:00 PM EST
  Qualification: Tier 1 (Score: 75)

  Quick Context:
  ‚Ä¢ Team of 8, evaluating for Q1 rollout
  ‚Ä¢ Current tool: DocuSign
  ‚Ä¢ Pain points: No Salesforce integration

  üîó Calendar: [link]
  üîó Salesforce: [link]
  üîó Prep Notes: [link]
  ```
- **Assignee**: Meeting owner auto-mentioned
- **Action**: Review prep notes before call

#### Alert: Competitor Mentioned
- **Trigger**: Competitor detected in conversation
- **Message Format**:
  ```
  ‚öîÔ∏è COMPETITOR MENTIONED
  Lead: John Smith (john@acme.com)
  Competitor: DocuSign
  Context: "Currently using DocuSign but frustrated with..."
  Sentiment: Negative toward competitor ‚úÖ

  üîó Full Transcript: [link]
  üîó Battlecard: [link to DocuSign battlecard]

  Opportunity: Switcher campaign
  ```
- **Action**: Use battlecard, position against competitor

### 7.4 Quality Alerts (Hourly digest - Slack #product)
**Severity**: P3 (Normal)
**Frequency**: Hourly batch at :00
**Channel**: Slack #product (no mentions)

#### Alert: Content Gaps Identified
- **Trigger**: Failed searches aggregated hourly
- **Message Format**:
  ```
  üìö CONTENT GAPS (Last Hour)

  Failed Searches (15 total):
  1. "How to use Zapier integration" (5 searches)
  2. "Mobile app availability" (3 searches)
  3. "Enterprise pricing details" (3 searches)

  üîó Full Report: [link to dashboard]

  Action: Update Intercom articles
  ```

#### Alert: Low Engagement Sessions
- **Trigger**: Sessions with engagement score <5
- **Message Format**:
  ```
  üìâ LOW ENGAGEMENT SESSIONS (Last Hour)

  3 calls with score <5:
  ‚Ä¢ Session abc123: Score 3.2 - High interruptions
  ‚Ä¢ Session def456: Score 4.1 - User frustration detected
  ‚Ä¢ Session ghi789: Score 4.8 - Abrupt ending

  üîó Review Transcripts: [link]

  Pattern: Users asking about feature X (not in KB)
  ```

#### Alert: Feature Requests
- **Trigger**: Feature requests mentioned in calls
- **Message Format**:
  ```
  üí° FEATURE REQUESTS (Last Hour)

  Requested Features (8 mentions):
  1. Mobile app (3 mentions) - "Need to sign on phone"
  2. Bulk send (2 mentions) - "Send 100 contracts at once"
  3. Custom branding (2 mentions) - "Remove PandaDoc logo"

  üîó Full Details: [link to request log]

  Action: Update product roadmap voting
  ```

### 7.5 Executive Alerts (Daily/Weekly - Email)
**Frequency**: Daily at 8 AM, Weekly on Monday
**Recipients**: VPs, Directors, Executives
**Format**: HTML email

#### Daily Executive Summary
**Subject**: "Voice Agent Daily Summary - [Date]"

**Content**:
```
PANDADOC VOICE AGENT DAILY SUMMARY
[Date]

üìä ACTIVITY
‚Ä¢ Total Calls: 45 (‚Üë 12% vs yesterday)
‚Ä¢ Qualified Leads: 12 (27% qualification rate)
‚Ä¢ Meetings Booked: 5 (42% booking rate)

‚ö° PERFORMANCE
‚Ä¢ Avg Latency: 987ms (‚úÖ target: <1000ms)
‚Ä¢ Engagement Score: 7.8/10 (‚Üë 0.3)
‚Ä¢ Error Rate: 1.2% (‚úÖ target: <5%)

üí∞ BUSINESS IMPACT
‚Ä¢ Pipeline Influenced: $127K (8 opportunities)
‚Ä¢ Hot Leads: 3 (scores >80)
‚Ä¢ Cost per Qualified Lead: $42

üî• TOP OPPORTUNITIES
1. Acme Corp - Score 92 - 25 users, Salesforce need
2. Globex Inc - Score 88 - 50 users, urgent timeline
3. Initech LLC - Score 85 - 15 users, DocuSign switcher

üìà TRENDS
‚Ä¢ Legal industry calls +40% this week
‚Ä¢ Salesforce integration requests +25%
‚Ä¢ "Mobile app" most requested feature

[View Full Dashboard ‚Üí]
```

#### Weekly Executive Rollup
**Subject**: "Voice Agent Weekly Review - Week of [Date]"

**Content**:
```
PANDADOC VOICE AGENT WEEKLY REVIEW
Week of [Date]

üìä WEEKLY SUMMARY
‚Ä¢ Total Calls: 287 (‚Üë 23% WoW)
‚Ä¢ Qualified Leads: 74 (26% rate, ‚Üë 2% WoW)
‚Ä¢ Meetings Booked: 31 (42% rate, ‚Üí flat)
‚Ä¢ Pipeline Influenced: $842K (47 opportunities)

üí∞ ROI METRICS
‚Ä¢ Total Cost: $1,247 (API + infrastructure)
‚Ä¢ Revenue per Call: $2,934 (pipeline / calls)
‚Ä¢ ROI: 67x (revenue / cost)

üéØ QUALIFICATION INSIGHTS
Top Qualification Drivers:
‚Ä¢ 58% had Salesforce integration need
‚Ä¢ 42% had teams of 5+ users
‚Ä¢ 31% mentioned competitor frustration

üìà GROWTH TRENDS
‚Ä¢ Call volume growing 23% WoW
‚Ä¢ Qualification rate stable at 26%
‚Ä¢ Meeting booking rate best-in-class 42%

‚ö†Ô∏è RISKS & OPPORTUNITIES
Risks:
‚Ä¢ Content gaps in Zapier integration docs
‚Ä¢ 15% of calls mention mobile app (not available)

Opportunities:
‚Ä¢ Legal industry surge (consider targeted campaign)
‚Ä¢ DocuSign switchers highly qualified (87% Tier 1)

üí° RECOMMENDATIONS
1. Create Zapier integration guide (15 failed searches)
2. Launch legal industry campaign (40% call growth)
3. Build DocuSign comparison page (mentioned in 23% of calls)

[View Full Dashboard ‚Üí]
[View Detailed Report ‚Üí]
```

---

## 8. Data Warehouse (Historical Storage)

### 8.1 Table Schemas

#### sessions (Fact Table)
**Purpose**: Core session metrics and metadata

**Columns**:
- `session_id` (STRING, PRIMARY KEY)
- `start_time` (TIMESTAMP)
- `end_time` (TIMESTAMP)
- `duration_seconds` (INTEGER)
- `participant_id` (STRING)
- `user_email` (STRING)
- `user_name` (STRING)
- `turn_count` (INTEGER)
- `interruption_count` (INTEGER)
- `qualification_tier` (STRING)
- `lead_score` (INTEGER)
- `engagement_score` (FLOAT)
- `sentiment` (STRING)
- `meeting_booked` (BOOLEAN)
- `created_at` (TIMESTAMP)

**Indexes**:
- `idx_user_email` on `user_email`
- `idx_start_time` on `start_time`
- `idx_qualification_tier` on `qualification_tier`

#### transcripts (Text Data)
**Purpose**: Full conversation transcripts

**Columns**:
- `session_id` (STRING, PRIMARY KEY, FOREIGN KEY ‚Üí sessions)
- `transcript_json` (JSON)
- `transcript_text` (TEXT) - Flattened for full-text search
- `word_count` (INTEGER)
- `created_at` (TIMESTAMP)

**Full-Text Index**: On `transcript_text` for search

#### metrics (Time-Series)
**Purpose**: Performance metrics over time

**Columns**:
- `id` (INTEGER, PRIMARY KEY, AUTO_INCREMENT)
- `session_id` (STRING, FOREIGN KEY ‚Üí sessions)
- `timestamp` (TIMESTAMP)
- `metric_type` (STRING) - 'latency', 'tokens', 'cost'
- `eou_delay_ms` (INTEGER)
- `llm_ttft_ms` (INTEGER)
- `tts_ttfb_ms` (INTEGER)
- `total_latency_ms` (INTEGER)
- `prompt_tokens` (INTEGER)
- `completion_tokens` (INTEGER)
- `total_tokens` (INTEGER)
- `stt_duration_ms` (INTEGER)
- `created_at` (TIMESTAMP)

**Indexes**:
- `idx_session_timestamp` on (`session_id`, `timestamp`)
- `idx_timestamp` on `timestamp` (for time-range queries)

#### leads (Dimension Table)
**Purpose**: Lead information and signals

**Columns**:
- `email` (STRING, PRIMARY KEY)
- `name` (STRING)
- `latest_session_id` (STRING, FOREIGN KEY ‚Üí sessions)
- `team_size` (INTEGER)
- `monthly_volume` (INTEGER)
- `integration_needs` (JSON ARRAY)
- `industry` (STRING)
- `location` (STRING)
- `use_case` (STRING)
- `current_tool` (STRING)
- `pain_points` (JSON ARRAY)
- `decision_timeline` (STRING)
- `budget_authority` (STRING)
- `urgency` (STRING)
- `qualification_tier` (STRING)
- `lead_score` (INTEGER)
- `first_call_date` (TIMESTAMP)
- `last_call_date` (TIMESTAMP)
- `total_calls` (INTEGER)
- `updated_at` (TIMESTAMP)

**Indexes**:
- `idx_qualification_tier` on `qualification_tier`
- `idx_lead_score` on `lead_score`
- `idx_last_call_date` on `last_call_date`

#### analysis_results (Processed Intelligence)
**Purpose**: Store analysis outputs

**Columns**:
- `session_id` (STRING, PRIMARY KEY, FOREIGN KEY ‚Üí sessions)
- `sales_intelligence` (JSON)
- `lead_score_breakdown` (JSON)
- `quality_analysis` (JSON)
- `sentiment_analysis` (JSON)
- `objections` (JSON)
- `competitors` (JSON)
- `performance_analysis` (JSON)
- `content_gaps` (JSON ARRAY)
- `action_items` (JSON ARRAY)
- `executive_summary` (TEXT)
- `analyzed_at` (TIMESTAMP)
- `analyzer_version` (STRING)

**Indexes**:
- `idx_analyzed_at` on `analyzed_at`

#### alerts_history (Audit Log)
**Purpose**: Track all alerts sent

**Columns**:
- `id` (INTEGER, PRIMARY KEY, AUTO_INCREMENT)
- `alert_type` (STRING) - 'critical', 'high', 'business', 'quality'
- `alert_name` (STRING) - Specific alert triggered
- `severity` (STRING) - 'P1', 'P2', 'P3'
- `message` (TEXT)
- `context` (JSON)
- `session_id` (STRING, NULLABLE, FOREIGN KEY ‚Üí sessions)
- `triggered_at` (TIMESTAMP)
- `sent_to` (STRING) - Channel/email
- `acknowledged_at` (TIMESTAMP, NULLABLE)
- `acknowledged_by` (STRING, NULLABLE)
- `resolved_at` (TIMESTAMP, NULLABLE)

**Indexes**:
- `idx_triggered_at` on `triggered_at`
- `idx_alert_type_severity` on (`alert_type`, `severity`)

### 8.2 Data Retention Policies

#### Raw Transcripts
- **Retention**: 90 days in hot storage
- **Archive**: After 90 days, move to cold storage (S3 Glacier, GCS Nearline)
- **Deletion**: After 2 years (compliance permitting)
- **PII Redaction**: Applied after 30 days for non-qualified leads
- **Access**: Restricted to authorized personnel only

#### Metrics
- **Retention**: 1 year at full granularity
- **Aggregation**: After 1 year, aggregate to hourly summaries
- **Archive**: Aggregated data retained for 5 years
- **Deletion**: After 5 years unless required for compliance

#### Analysis Results
- **Retention**: Indefinite
- **Reason**: Business intelligence, model training, trend analysis
- **PII**: Redacted or anonymized per privacy policy

#### Alerts History
- **Retention**: 1 year in active database
- **Archive**: After 1 year, move to cold storage
- **Deletion**: After 3 years (audit log requirements)

#### PII Data Handling (GDPR/CCPA Compliance)
- **Redaction Policy**:
  - Tier 1 leads: PII retained until deal closed + 1 year
  - Tier 2 leads: PII redacted after 30 days
  - Deleted accounts: PII purged within 30 days of deletion request
- **Redacted Fields**:
  - Email ‚Üí hashed
  - Name ‚Üí "User [hash]"
  - Specific quotes ‚Üí "[REDACTED]"
  - Company names ‚Üí generalized industry
- **Exceptions**: Aggregated analytics (fully anonymized) retained indefinitely

### 8.3 Data Access Patterns

#### Real-Time Queries (Dashboard)
- **Target**: Last 24 hours of data
- **Latency**: <1 second
- **Cache**: 5-minute cache for aggregate metrics
- **Optimization**: Materialized views for common queries

#### Analytical Queries (Reports)
- **Target**: Last 90 days typically
- **Latency**: <30 seconds acceptable
- **Schedule**: Run overnight for large reports
- **Optimization**: Columnar storage, partitioning by date

#### Historical Analysis (Trends)
- **Target**: Up to 1 year of data
- **Latency**: <2 minutes acceptable
- **Access**: Typically scheduled jobs
- **Optimization**: Pre-aggregated rollups

---

## Summary: Complete Data Lineage

### Data Flow Summary
1. **LiveKit Agent** collects 70+ raw data points during conversation
2. **Message Queue** reliably transports data to analytics
3. **Analytics Agent** processes data through 9 specialized modules
4. **Salesforce** receives 40+ enriched fields for sales action
5. **Amplitude** tracks 15+ events and properties for product analytics
6. **Dashboard** displays 35+ metrics across 6 panels in real-time
7. **Alerts** notify 4 teams with 20+ alert types across 5 severity levels
8. **Data Warehouse** stores 6 table schemas with full retention policies

### Total Data Points Tracked
- **Collection**: 70+ metrics and signals
- **Analysis**: 50+ derived insights
- **Distribution**: 100+ fields across systems
- **Visualization**: 35+ dashboard metrics
- **Alerting**: 20+ alert types

---

*Document Version: 1.0*
*Last Updated: 2025-01-28*
*Companion to: ANALYTICS_STRATEGY.md*
*Purpose: Complete data flow specification for implementation*